{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b49f4e35",
   "metadata": {},
   "source": [
    "# 12. 딥러닝 들여다보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b434d6d8",
   "metadata": {},
   "source": [
    "## 12-2. 신경망 구성 (1) 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1155d43f",
   "metadata": {},
   "source": [
    "### MNIST Revisited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f1d2e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 39,760\n",
      "Trainable params: 39,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4930 - accuracy: 0.8813\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2336 - accuracy: 0.9333\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1814 - accuracy: 0.9487\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1499 - accuracy: 0.9577\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1274 - accuracy: 0.9641\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1108 - accuracy: 0.9687\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0980 - accuracy: 0.9724\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0872 - accuracy: 0.9757\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0791 - accuracy: 0.9776\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0717 - accuracy: 0.9801\n",
      "313/313 - 0s - loss: 0.1078 - accuracy: 0.9680\n",
      "test_loss: 0.10780690610408783 \n",
      "test_accuracy: 0.9679999947547913\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# MNIST 데이터를 로드. 다운로드하지 않았다면 다운로드까지 자동으로 진행됩니다. \n",
    "mnist = keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()   \n",
    "\n",
    "# 모델에 맞게 데이터 가공\n",
    "x_train_norm, x_test_norm = x_train / 255.0, x_test / 255.0\n",
    "x_train_reshaped = x_train_norm.reshape(-1, x_train_norm.shape[1]*x_train_norm.shape[2])\n",
    "x_test_reshaped = x_test_norm.reshape(-1, x_test_norm.shape[1]*x_test_norm.shape[2])\n",
    "\n",
    "# 딥러닝 모델 구성 - 2 Layer Perceptron\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(50, activation='sigmoid', input_shape=(784,)))  # 입력층 d=784, 은닉층 레이어 H=50\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))   # 출력층 레이어 K=10\n",
    "model.summary()\n",
    "\n",
    "# 모델 구성과 학습\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "model.fit(x_train_reshaped, y_train, epochs=10)\n",
    "\n",
    "# 모델 테스트 결과\n",
    "test_loss, test_accuracy = model.evaluate(x_test_reshaped,y_test, verbose=2)\n",
    "print(f\"test_loss: {test_loss} \")\n",
    "print(f\"test_accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46408e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "# 입력층 데이터의 모양(shape)\n",
    "print(x_train_reshaped.shape)\n",
    "\n",
    "# 테스트를 위해 x_train_reshaped의 앞 5개의 데이터를 가져온다.\n",
    "X = x_train_reshaped[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab2455b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1.shape: (784, 50)\n",
      "X.shape: (5, 784)\n",
      "b1.shape: (50,)\n",
      "a1.shape: (5, 50)\n"
     ]
    }
   ],
   "source": [
    "weight_init_std = 0.1\n",
    "input_size = 784\n",
    "hidden_size=50\n",
    "\n",
    "# 인접 레이어간 관계를 나타내는 파라미터 W를 생성하고 random 초기화\n",
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)  \n",
    "# 바이어스 파라미터 b를 생성하고 Zero로 초기화\n",
    "b1 = np.zeros(hidden_size)\n",
    "\n",
    "a1 = np.dot(X, W1) + b1   # 은닉층 출력\n",
    "\n",
    "print(f'W1.shape:',W1.shape)\n",
    "print(f'X.shape:',X.shape)\n",
    "print(f'b1.shape:', b1.shape)\n",
    "print(f'a1.shape:',a1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00daf32e",
   "metadata": {},
   "source": [
    "<span style = \"color:green; font-size:150%\"> \n",
    "<b>Q. 첫 번째 데이터의 은닉층 출력을 확인해 봅시다.  50dim의 벡터가 나오나요?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b9b4194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03903154,  1.95237002, -1.23297817,  0.7774001 ,  0.98683625,\n",
       "       -0.792677  ,  1.3778527 ,  0.47579772,  0.59490477,  0.56227145,\n",
       "        1.07699341, -0.94802499, -0.80901878,  1.66652714,  0.52353009,\n",
       "        0.19155294,  0.78148131,  1.52314006,  1.82990768,  0.25271733,\n",
       "       -0.75604994,  0.61529505,  1.69565685, -0.17649813,  0.82615092,\n",
       "        0.41822442,  0.17899841,  0.48505913, -3.32445536,  0.28745757,\n",
       "        2.24056095, -0.94598001, -0.29811733,  0.34079453,  0.38055451,\n",
       "       -1.23991348,  1.99856564,  1.20417594, -1.0651277 , -0.84259215,\n",
       "        1.56570276,  0.03720417,  1.18520024, -1.85563565,  0.42752201,\n",
       "        1.97149099, -0.36325767,  1.0874221 , -0.6808314 ,  0.50196858])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 첫 번째 데이터의 은닉층 출력을 확인해 봅시다.  50dim의 벡터가 나오나요?\n",
    "a1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ae3c08",
   "metadata": {},
   "source": [
    "## 12-3. 신경망 구성 (2) 활성화 함수와 손실 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2605c15",
   "metadata": {},
   "source": [
    "## 활성화 함수 (Activation Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec84deb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49024335 0.87570484 0.2256606  0.68511951 0.72846257 0.31159415\n",
      " 0.79864591 0.61675508 0.64448973 0.63697795 0.74592459 0.27928219\n",
      " 0.30809963 0.84111225 0.62797285 0.54774234 0.68599928 0.82100041\n",
      " 0.86175073 0.56284522 0.31950448 0.64914773 0.84496664 0.45598966\n",
      " 0.69554045 0.60305829 0.5446305  0.6189418  0.03474169 0.57137359\n",
      " 0.90383323 0.27969399 0.42601778 0.58438351 0.59400684 0.22445105\n",
      " 0.8806464  0.76926683 0.25633076 0.30098913 0.82717014 0.50929997\n",
      " 0.76588153 0.13521256 0.60528179 0.87777117 0.41017121 0.74789598\n",
      " 0.33607577 0.62292184]\n"
     ]
    }
   ],
   "source": [
    "# 위 수식의 sigmoid 함수를 구현해 봅니다.\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))  \n",
    "\n",
    "\n",
    "z1 = sigmoid(a1)\n",
    "print(z1[0])  # sigmoid의 출력은 모든 element가 0에서 1사이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6bde9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "푸슝~3\n"
     ]
    }
   ],
   "source": [
    "# 단일 레이어 구현 함수\n",
    "def affine_layer_forward(X, W, b):\n",
    "    y = np.dot(X, W) + b\n",
    "    cache = (X, W, b)\n",
    "    return y, cache\n",
    "\n",
    "print('푸슝~3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70f1d769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.28615831  0.07776111  0.75183561  0.20877389  0.04245447 -0.52126731\n",
      " -0.60078501 -1.00608647 -0.03605047 -0.60720834]\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "hidden_size = 50\n",
    "output_size = 10\n",
    "\n",
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros(hidden_size)\n",
    "W2 = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros(output_size)\n",
    "\n",
    "a1, cache1 = affine_layer_forward(X, W1, b1)\n",
    "z1 = sigmoid(a1)\n",
    "a2, cache2 = affine_layer_forward(z1, W2, b2)    # z1이 다시 두번째 레이어의 입력이 됩니다. \n",
    "\n",
    "print(a2[0])  # 최종 출력이 output_size만큼의 벡터가 되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eb11d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b47ababa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08124258, 0.11690468, 0.22939225, 0.13326927, 0.11284918,\n",
       "       0.06422101, 0.05931207, 0.03954785, 0.10432879, 0.05893231])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = softmax(a2)\n",
    "y_hat[0]  # 10개의 숫자 중 하나일 확률이 되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ac1a994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정답 라벨을 One-hot 인코딩하는 함수\n",
    "def _change_one_hot_label(X, num_category):\n",
    "    T = np.zeros((X.size, num_category))\n",
    "    for idx, row in enumerate(T):\n",
    "        row[X[idx]] = 1\n",
    "        \n",
    "    return T\n",
    "\n",
    "Y_digit = y_train[:5]\n",
    "t = _change_one_hot_label(Y_digit, 10)\n",
    "t     # 정답 라벨의 One-hot 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e31dbd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08124258 0.11690468 0.22939225 0.13326927 0.11284918 0.06422101\n",
      " 0.05931207 0.03954785 0.10432879 0.05893231]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_hat[0])\n",
    "print(t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b39d820a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.568821561442879"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size\n",
    "\n",
    "Loss = cross_entropy_error(y_hat, t)\n",
    "Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a41aa0c",
   "metadata": {},
   "source": [
    "## 12-4. 경사하강법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fba99d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01624852,  0.02338094,  0.04587845,  0.02665385,  0.02256984,\n",
       "        -0.1871558 ,  0.01186241,  0.00790957,  0.02086576,  0.01178646],\n",
       "       [-0.18162585,  0.01956423,  0.04788864,  0.02375783,  0.02388226,\n",
       "         0.01265623,  0.01267405,  0.00842566,  0.02060655,  0.0121704 ],\n",
       "       [ 0.01627226,  0.02360201,  0.04363288,  0.02702262, -0.17821905,\n",
       "         0.01356558,  0.01445195,  0.00811647,  0.02082361,  0.01073167],\n",
       "       [ 0.01653792, -0.17985412,  0.04743667,  0.02872768,  0.02020412,\n",
       "         0.01531879,  0.01053251,  0.00744132,  0.02278625,  0.01086886],\n",
       "       [ 0.01933804,  0.02206629,  0.04957146,  0.02450896,  0.02691587,\n",
       "         0.01356752,  0.01273533,  0.00646114,  0.01667243, -0.19183704]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_num = y_hat.shape[0]\n",
    "dy = (y_hat - t) / batch_num\n",
    "dy    # softmax값의 출력으로 Loss를 미분한 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5216d32d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08374543, -0.10109025,  0.16569379,  0.09173103, -0.00825372,\n",
       "        -0.08594283,  0.04287603,  0.02666851,  0.07140906, -0.1193462 ],\n",
       "       [-0.07390999, -0.00872776,  0.12052884,  0.06846813, -0.10091666,\n",
       "        -0.1017346 ,  0.03328316,  0.02068431,  0.05410746, -0.0117829 ],\n",
       "       [-0.11413739, -0.06111407,  0.18277707,  0.10113685, -0.08382722,\n",
       "        -0.05362497,  0.04895666,  0.02987097,  0.07878913, -0.12882704],\n",
       "       [-0.08543287, -0.0190473 ,  0.09660665,  0.05365513, -0.00779545,\n",
       "        -0.10632293,  0.02545976,  0.01627549,  0.04272643, -0.01612491],\n",
       "       [-0.11087242,  0.02784476,  0.12254285,  0.0672307 , -0.0700382 ,\n",
       "        -0.07684512,  0.03370296,  0.02054433,  0.05282884, -0.0669387 ],\n",
       "       [-0.05085929,  0.00989253,  0.08903689,  0.04922727, -0.04702115,\n",
       "        -0.06663896,  0.02428489,  0.0147263 ,  0.03825346, -0.06090194],\n",
       "       [-0.09532622, -0.08221955,  0.17370211,  0.09708879, -0.04026627,\n",
       "        -0.12324761,  0.04567141,  0.02859816,  0.07612425, -0.08012505],\n",
       "       [-0.02654114, -0.04093685,  0.09817517,  0.05550501, -0.03587077,\n",
       "        -0.09002134,  0.02602225,  0.01619185,  0.04320874, -0.04573293],\n",
       "       [-0.04618558, -0.02491046,  0.10095496,  0.05607627, -0.03964241,\n",
       "        -0.06088766,  0.02698375,  0.01648255,  0.04349824, -0.07236966],\n",
       "       [-0.08591576, -0.03955522,  0.11982217,  0.06673875, -0.04039775,\n",
       "        -0.07802488,  0.03188933,  0.01992864,  0.05258521, -0.04707049],\n",
       "       [-0.06972073, -0.07051869,  0.14625939,  0.08064169, -0.03404893,\n",
       "        -0.04624814,  0.03842553,  0.02342895,  0.06235265, -0.13057172],\n",
       "       [-0.00582576, -0.00793126,  0.07912419,  0.0448645 , -0.08692557,\n",
       "        -0.03073171,  0.02208164,  0.01304381,  0.03433075, -0.06203059],\n",
       "       [-0.03361305,  0.00327963,  0.08631154,  0.04861325, -0.08304186,\n",
       "        -0.04910239,  0.0240529 ,  0.01445426,  0.0377012 , -0.04865547],\n",
       "       [ 0.00209874, -0.08899292,  0.14978856,  0.08524422, -0.07274636,\n",
       "        -0.1057972 ,  0.03968175,  0.02432528,  0.0655853 , -0.09918737],\n",
       "       [-0.04227321, -0.03540426,  0.09453513,  0.05258819, -0.04945051,\n",
       "        -0.02822199,  0.02535542,  0.01537182,  0.04071579, -0.07321637],\n",
       "       [-0.07383008, -0.01630349,  0.08000956,  0.04306741,  0.00663609,\n",
       "        -0.03688446,  0.02087619,  0.01286864,  0.03367111, -0.07011098],\n",
       "       [-0.05726298, -0.07659703,  0.1476635 ,  0.08207178, -0.01511807,\n",
       "        -0.0992538 ,  0.03842139,  0.02381102,  0.06367015, -0.10740597],\n",
       "       [-0.04841995, -0.029489  ,  0.11719813,  0.0648314 , -0.05421566,\n",
       "        -0.0451255 ,  0.03143998,  0.0189389 ,  0.04996475, -0.10512306],\n",
       "       [-0.10125359, -0.05917382,  0.15083048,  0.0822349 , -0.01010179,\n",
       "        -0.05362865,  0.03939914,  0.0241465 ,  0.06386946, -0.13632262],\n",
       "       [-0.0272626 , -0.01237153,  0.08239679,  0.04670978, -0.08090039,\n",
       "        -0.0397079 ,  0.02284422,  0.01379056,  0.03625743, -0.04175636],\n",
       "       [-0.11427449, -0.04914555,  0.13629622,  0.07454241, -0.00905924,\n",
       "        -0.06594141,  0.03568005,  0.02220333,  0.05853994, -0.08884126],\n",
       "       [-0.02620879, -0.03904748,  0.07517242,  0.04241463, -0.04396321,\n",
       "        -0.03185867,  0.02012858,  0.01235495,  0.03297806, -0.04197049],\n",
       "       [-0.0270757 , -0.07468486,  0.15395918,  0.08665119, -0.0725086 ,\n",
       "        -0.08529212,  0.0409468 ,  0.024965  ,  0.0667826 , -0.1137435 ],\n",
       "       [-0.12196266, -0.08716205,  0.13144943,  0.07302016, -0.0063818 ,\n",
       "        -0.07228179,  0.03404955,  0.02182931,  0.05819791, -0.03075807],\n",
       "       [-0.08606323, -0.06092624,  0.13184096,  0.07346454, -0.0338644 ,\n",
       "        -0.07962173,  0.03475357,  0.02175182,  0.05775334, -0.05908861],\n",
       "       [-0.07929914, -0.08237653,  0.1710824 ,  0.09505154, -0.03520517,\n",
       "        -0.09691899,  0.04487287,  0.0277318 ,  0.07390084, -0.11883962],\n",
       "       [-0.11289058, -0.03999339,  0.14739451,  0.08134532, -0.00947317,\n",
       "        -0.12540086,  0.03867844,  0.02432987,  0.0640958 , -0.06808594],\n",
       "       [-0.04259479,  0.01321016,  0.12442182,  0.06790941, -0.06130282,\n",
       "        -0.05400228,  0.0338088 ,  0.01985164,  0.05165582, -0.15295777],\n",
       "       [-0.09412682, -0.02154861,  0.09806325,  0.0542958 , -0.02864108,\n",
       "        -0.06791033,  0.02616442,  0.01648504,  0.04316695, -0.02594863],\n",
       "       [-0.01897623, -0.02109007,  0.08755131,  0.04904868, -0.0351511 ,\n",
       "        -0.07316677,  0.02338179,  0.01426887,  0.03782411, -0.06369057],\n",
       "       [-0.01937362, -0.09130328,  0.10360267,  0.05940729, -0.08686886,\n",
       "        -0.00666404,  0.02780129,  0.01703658,  0.0461417 , -0.04977973],\n",
       "       [-0.11125934, -0.065159  ,  0.13992305,  0.07605906, -0.00270361,\n",
       "        -0.03442804,  0.03638469,  0.02241946,  0.05934452, -0.1205808 ],\n",
       "       [-0.03186616, -0.04664927,  0.12001614,  0.06820448, -0.10160425,\n",
       "        -0.05185636,  0.0327536 ,  0.01991786,  0.05292166, -0.06183771],\n",
       "       [-0.0946238 , -0.07173997,  0.15224962,  0.08443962,  0.00535497,\n",
       "        -0.13194267,  0.03940926,  0.02491488,  0.06633137, -0.07439329],\n",
       "       [-0.11479172, -0.08128955,  0.13748427,  0.07705568, -0.02751948,\n",
       "        -0.09355133,  0.03600782,  0.0230613 ,  0.06140301, -0.01785998],\n",
       "       [-0.0707016 , -0.0099235 ,  0.09280128,  0.05082922, -0.03260555,\n",
       "        -0.04224203,  0.02490526,  0.01519918,  0.03967635, -0.06793861],\n",
       "       [ 0.00550353, -0.10411284,  0.07501206,  0.04449936, -0.08128236,\n",
       "        -0.00467526,  0.02005641,  0.01257425,  0.03481684, -0.00239198],\n",
       "       [-0.05114952, -0.0460285 ,  0.10466013,  0.0591065 , -0.07997482,\n",
       "        -0.0318742 ,  0.02841982,  0.0174188 ,  0.04620452, -0.04678273],\n",
       "       [-0.0615911 , -0.01726555,  0.07706489,  0.04295162, -0.05197479,\n",
       "        -0.02608563,  0.02100346,  0.0129241 ,  0.03384548, -0.03087248],\n",
       "       [-0.07527313, -0.03218454,  0.11433057,  0.06298315, -0.0586258 ,\n",
       "        -0.01873599,  0.03078017,  0.01862503,  0.04895917, -0.09085862],\n",
       "       [-0.02575564, -0.04838301,  0.13928211,  0.07882584, -0.08315755,\n",
       "        -0.09934919,  0.03749202,  0.0229101 ,  0.06095809, -0.08282278],\n",
       "       [-0.11854148, -0.01848785,  0.15720634,  0.08559995, -0.03365253,\n",
       "        -0.07400923,  0.04183062,  0.0255046 ,  0.0666327 , -0.13208313],\n",
       "       [-0.03014598, -0.06004386,  0.1206617 ,  0.06772058, -0.04954608,\n",
       "        -0.06554056,  0.031978  ,  0.01956899,  0.05232071, -0.08697349],\n",
       "       [-0.10834087, -0.07177   ,  0.1706912 ,  0.09445279, -0.02113914,\n",
       "        -0.11065385,  0.04469093,  0.02788164,  0.07396386, -0.09977656],\n",
       "       [-0.11749113, -0.09989316,  0.18146137,  0.10107478, -0.02227647,\n",
       "        -0.12071518,  0.04730695,  0.02983251,  0.07954281, -0.07884248],\n",
       "       [-0.05959068, -0.02600143,  0.10448895,  0.05851129, -0.0714476 ,\n",
       "        -0.04427269,  0.02842902,  0.01737935,  0.04572717, -0.05322338],\n",
       "       [-0.10911621, -0.03416661,  0.14279336,  0.079373  , -0.0629726 ,\n",
       "        -0.08113618,  0.03836217,  0.02381228,  0.06252392, -0.05947313],\n",
       "       [-0.04406696, -0.07293042,  0.14889774,  0.0830479 , -0.01752236,\n",
       "        -0.11691474,  0.03879288,  0.02402617,  0.06428975, -0.10761997],\n",
       "       [-0.09122786, -0.05433746,  0.095352  ,  0.05245862, -0.01257656,\n",
       "        -0.02733528,  0.02490612,  0.01566951,  0.04154768, -0.04445678],\n",
       "       [-0.10506388, -0.09190826,  0.16574523,  0.09135072,  0.01237297,\n",
       "        -0.10615974,  0.04268045,  0.02679068,  0.07149205, -0.10730023]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW2 = np.dot(z1.T, dy)    \n",
    "dW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9120967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dW2 = np.dot(z1.T, dy)\n",
    "db2 = np.sum(dy, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af6b2574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_grad(x):\n",
    "    return (1.0 - sigmoid(x)) * sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87905427",
   "metadata": {},
   "outputs": [],
   "source": [
    "dz1 = np.dot(dy, W2.T)\n",
    "da1 = sigmoid_grad(a1) * dz1\n",
    "dW1 = np.dot(X.T, da1)\n",
    "db1 = np.sum(dz1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83b7c09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    W1 = W1 - learning_rate*dW1\n",
    "    b1 = b1 - learning_rate*db1\n",
    "    W2 = W2 - learning_rate*dW2\n",
    "    b2 = b2 - learning_rate*db2\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2438ecb",
   "metadata": {},
   "source": [
    "## 2-5. 오차역전파법이란?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed52163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_layer_backward(dy, cache):\n",
    "    X, W, b = cache\n",
    "    dX = np.dot(dy, W.T)\n",
    "    dW = np.dot(X.T, dy)\n",
    "    db = np.sum(dy, axis=0)\n",
    "    return dX, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c379f402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.08028677 0.06087052 0.08344577 0.11040949 0.05554446 0.11852898\n",
      "  0.15350876 0.09561458 0.05476104 0.18702962]\n",
      " [0.1016505  0.06378328 0.08203009 0.105985   0.05120713 0.13454029\n",
      "  0.14194351 0.11028396 0.05217102 0.15640522]\n",
      " [0.0851558  0.06356971 0.08847663 0.09885321 0.05675833 0.14484107\n",
      "  0.13038777 0.09239194 0.04760213 0.19196342]\n",
      " [0.0887097  0.05853812 0.07841847 0.10751438 0.06407255 0.12552467\n",
      "  0.11938749 0.11441481 0.06117691 0.1822429 ]\n",
      " [0.10985657 0.05697674 0.0878311  0.09822158 0.05057798 0.13862127\n",
      "  0.15189127 0.09627654 0.05836421 0.15138274]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  2.4027572754628346\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 초기화\n",
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros(hidden_size)\n",
    "W2 = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros(output_size)\n",
    "\n",
    "# Forward Propagation\n",
    "a1, cache1 = affine_layer_forward(X, W1, b1)\n",
    "z1 = sigmoid(a1)\n",
    "a2, cache2 = affine_layer_forward(z1, W2, b2)\n",
    "\n",
    "# 추론과 오차(Loss) 계산\n",
    "y_hat = softmax(a2)\n",
    "t = _change_one_hot_label(Y_digit, 10)   # 정답 One-hot 인코딩\n",
    "Loss = cross_entropy_error(y_hat, t)\n",
    "\n",
    "print(y_hat)\n",
    "print(t)\n",
    "print('Loss: ', Loss)\n",
    "        \n",
    "dy = (y_hat - t) / X.shape[0]\n",
    "dz1, dW2, db2 = affine_layer_backward(dy, cache2)\n",
    "da1 = sigmoid_grad(a1) * dz1\n",
    "dX, dW1, db1 = affine_layer_backward(da1, cache1)\n",
    "\n",
    "# 경사하강법을 통한 파라미터 업데이트    \n",
    "learning_rate = 0.1\n",
    "W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2212e6",
   "metadata": {},
   "source": [
    "## 12-6. 모델 학습 Step-by-Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31aefbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros(hidden_size)\n",
    "W2 = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros(output_size)\n",
    "\n",
    "def train_step(X, Y, W1, b1, W2, b2, learning_rate=0.1, verbose=False):\n",
    "    a1, cache1 = affine_layer_forward(X, W1, b1)\n",
    "    z1 = sigmoid(a1)\n",
    "    a2, cache2 = affine_layer_forward(z1, W2, b2)\n",
    "    y_hat = softmax(a2)\n",
    "    t = _change_one_hot_label(Y, 10)\n",
    "    Loss = cross_entropy_error(y_hat, t)\n",
    "\n",
    "    if verbose:\n",
    "        print('---------')\n",
    "        print(y_hat)\n",
    "        print(t)\n",
    "        print('Loss: ', Loss)\n",
    "        \n",
    "    dy = (y_hat - t) / X.shape[0]\n",
    "    dz1, dW2, db2 = affine_layer_backward(dy, cache2)\n",
    "    da1 = sigmoid_grad(a1) * dz1\n",
    "    dX, dW1, db1 = affine_layer_backward(da1, cache1)\n",
    "    \n",
    "    W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "    \n",
    "    return W1, b1, W2, b2, Lossb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44977f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "[[0.10132424 0.04694082 0.1593532  0.07336761 0.05198382 0.07863765\n",
      "  0.14074116 0.18650205 0.07199905 0.0891504 ]\n",
      " [0.09371504 0.04729875 0.17022429 0.08511428 0.061696   0.07386892\n",
      "  0.12763935 0.19195799 0.07288136 0.07560401]\n",
      " [0.09392293 0.05261715 0.15017474 0.08098165 0.04561189 0.08376934\n",
      "  0.13906608 0.17435851 0.08082462 0.09867309]\n",
      " [0.09327881 0.05243794 0.15517492 0.07971568 0.0445909  0.07672965\n",
      "  0.17316253 0.1804449  0.07106796 0.0733967 ]\n",
      " [0.0991319  0.05225459 0.15501479 0.07882603 0.0580723  0.06916437\n",
      "  0.16237059 0.16714562 0.08093793 0.07708188]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  2.7018000060266028\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Lossb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30/4199460364.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# train_step을 다섯 번 반복 돌립니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_30/1283629775.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(X, Y, W1, b1, W2, b2, learning_rate, verbose)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLossb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Lossb' is not defined"
     ]
    }
   ],
   "source": [
    "X = x_train_reshaped[:5]\n",
    "Y = y_train[:5]\n",
    "\n",
    "# train_step을 다섯 번 반복 돌립니다.\n",
    "for i in range(5):\n",
    "    W1, b1, W2, b2, _ = train_step(X, Y, W1, b1, W2, b2, learning_rate=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2879ca73",
   "metadata": {},
   "source": [
    "## 12-7. 추론 과정 구현과 정확도(Accuracy) 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99323fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = x_train[:100] 에 대해 모델 추론을 시도합니다. \n",
    "X = x_train_reshaped[:100]\n",
    "Y = y_test[:100]\n",
    "result = predict(W1, b1, W2, b2, X)\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2766ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(W1, b1, W2, b2, x, y):\n",
    "    y_hat = predict(W1, b1, W2, b2, x)\n",
    "    y_hat = np.argmax(y_hat, axis=1)\n",
    "\n",
    "    accuracy = np.sum(y_hat == y) / float(x.shape[0])\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf04c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy(W1, b1, W2, b2, X, Y)\n",
    "\n",
    "t = _change_one_hot_label(Y, 10)\n",
    "print(result[0])\n",
    "print(t[0])\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6827a7",
   "metadata": {},
   "source": [
    "## 12-8. 전체 학습 사이클 수행 - 종합문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b969690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# mnist를 불러오고 train_data, train_label, test_data, test_label로 나눠주세요.\n",
    "\n",
    "#우리의 모델은 MLP이기 때문에 데이터를 255로 나누고 1차원(60000, n)으로 만들어주세요.\n",
    "\n",
    "# 초기화된 파라미터를 정의하는 함수를 만들고 초기값을 만드세요.\n",
    "def init_params(input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "    #W1, b1, W2, b2를 모두 정의해주세요.\n",
    "    \n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "W1, b1, W2, b2 = init_params(input_size = \"YOUR CODE\", hidden_size = 50, output_size = \"YOUR CODE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fb8ef5",
   "metadata": {},
   "source": [
    "<span style = \"color:green; font-size:150%\"> \n",
    "<b>Q. MLP를 정의하세요.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6a692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP를 정의하세요.\n",
    "def affine_layer_forward(X, W, b):\n",
    "  #[[YOUR CODE]]\n",
    "  return y, cache\n",
    "\n",
    "# relu를 정의하세요 (np.maximum을 활용하세요)\n",
    "def relu(x):\n",
    "  #[[YOUR CODE]]\n",
    "  return result\n",
    "\n",
    "# softmax를 정의하세요\n",
    "def softmax(x):\n",
    "  #[[YOUR CODE]]\n",
    "  return result\n",
    "\n",
    "# one-hot 인코딩을 정의하세요\n",
    "def _change_one_hot_label(X, num_category):\n",
    "    #[[YOUR CODE]]\n",
    "    return T\n",
    "\n",
    "# cross entropy loss함수를 정의하세요\n",
    "def cross_entropy_error(y, t):\n",
    "     #[[YOUR CODE]]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caf3718",
   "metadata": {},
   "source": [
    "<span style = \"color:green; font-size:150%\"> \n",
    "<b>Q. MLP의 backward pass를 정의하세요</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3a7e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP의 backward pass를 정의하세요\n",
    "def affine_layer_backward(dy, cache):\n",
    "  #[[YOUR CODE]]\n",
    "  return dX, dW, db\n",
    "\n",
    "# relu 함수의 backward pass를 정의하세요. (np.where 함수를 활용하세요)\n",
    "def relu_grad(x):\n",
    "  #[[YOUR CODE]]\n",
    "  return result\n",
    "\n",
    "#파라미터를 업데이트하는 함수를 정의하세요.\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "  #[[YOUR CODE]]\n",
    "  return W1, b1, W2, b2\n",
    "\n",
    "# train_step을 정의합니다.\n",
    "def train_step(X, Y, W1, b1, W2, b2, learning_rate=0.1, verbose=False):\n",
    "  #[[YOUR CODE]]\n",
    "\n",
    "  if verbose:\n",
    "      print('---------')\n",
    "      print(y_hat)\n",
    "      print(t)\n",
    "      print('Loss: ', Loss)\n",
    "      \n",
    "  #[[YOUR CODE]]\n",
    "  \n",
    "  W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "  \n",
    "  return W1, b1, W2, b2, Loss\n",
    "\n",
    "# 예측값을 만드는 함수를 정의하세요\n",
    "def predict(W1, b1, W2, b2, X):\n",
    "  #[[YOUR CODE]]\n",
    "  return y\n",
    "\n",
    "#정확도를 나타내는 함수를 정의하세요\n",
    "def accuracy(W1, b1, W2, b2, x, y):\n",
    "  #[[YOUR CODE]]\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403e945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터\n",
    "iters_num = 50000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "W1, b1, W2, b2 = init_params(784, 50, 10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "  # 미니배치 획득\n",
    "  batch_mask = np.random.choice(train_size, batch_size)\n",
    "  x_batch = x_train_reshaped[batch_mask]\n",
    "  y_batch = y_train[batch_mask]\n",
    "  \n",
    "\n",
    "  W1, b1, W2, b2, Loss = train_step(\"YOUR CODE\")\n",
    "\n",
    "  # 학습 경과 기록\n",
    "  train_loss_list.append(Loss)\n",
    "  \n",
    "  # 1에폭당 정확도 계산\n",
    "  # train_accuracy와 test_accuracy를 완성해주세요\n",
    "  if i % iter_per_epoch == 0:\n",
    "      print('Loss: ', Loss)\n",
    "      train_acc = accuracy(\"YOUR CODE\")\n",
    "      test_acc = accuracy(\"YOUR CODE\")\n",
    "      train_acc_list.append(train_acc)\n",
    "      test_acc_list.append(test_acc)\n",
    "      print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac540615",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 6 \n",
    "\n",
    "# Accuracy 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c584bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss 그래프 그리기\n",
    "x = np.arange(len(train_loss_list))\n",
    "plt.plot(x, train_loss_list, label='train acc')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim(0, 3.0)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
